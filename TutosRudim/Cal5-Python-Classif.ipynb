{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<a href=\"http://www.insa-toulouse.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/logo-insa.jpg\" style=\"float:left; max-width: 120px; display: inline\" alt=\"INSA\"/></a> \n",
    "\n",
    "<a href=\"http://wikistat.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/wikistat.jpg\" style=\"float:right; max-width: 250px; display: inline\"  alt=\"Wikistat\"/></a>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Scénarios d'Exploration Statistique](https://github.com/wikistat/Exploration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification non supervisée avec <a href=\"https://www.python.org/\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/f/f8/Python_logo_and_wordmark.svg/390px-Python_logo_and_wordmark.svg.png\" style=\"max-width: 150px; display: inline\" alt=\"Python\"/></a> \n",
    "\n",
    "**Résumé**: Illustration des algorithmes de [classification non supervisée]() de `Scikit-learn` sur des données \"jouet\": réallocation dynamique ($k$-means), DBSCAN, mélange gaussien.\n",
    "\n",
    "Tutoriel adapté de: https://github.com/ageron/handson-ml/blob/master/08_dimensionality_reduction.ipynb\n",
    "\n",
    "*Machine Learning avec Scikit-Learn*, Mise en oeuvre et cas concrets, par Aurélien Géron."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Introduction\n",
    "### 1.1 Objectif\n",
    "Ce tutoriel permet d'illustrer sur des données simulées ou \"jouet\"les principales fonctions de classification non supervisées présentent dans la librairie `Scikit-learn`. L'algorithme $k$-means y est particulièrement développé ainsi que différentes versions de DBSCAN et les modèles de mélange gaussien. En revanche, la classification ascendante hiérarchique est à rechercher dans la librairie `scipy` L'usage de cet algorrithme est illustré dans un autre tutoriel.\n",
    " \n",
    "### 1.2  Environnement pour python 3 ou 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# initialisation du générateur de nombres aléatoires\n",
    "np.random.seed(42)\n",
    "# Graphiques\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "# Ignorer les warnings inutiles (cf. SciPy issue #5998)\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\", module=\"scipy\", message=\"^internal gelsd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 *K-means*\n",
    "Cet algorithme et quelques unes de ses variantes sont testées sur des données simulées.\n",
    "\n",
    "### 2.1 Génération des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition des centres\n",
    "blob_centers = np.array(\n",
    "    [[ 0.2,  2.3],\n",
    "     [-1.5 ,  2.3],\n",
    "     [-2.8,  1.8],\n",
    "     [-2.8,  2.8],\n",
    "     [-2.8,  1.3]])\n",
    "# Définition des dispersions\n",
    "blob_std = np.array([0.4, 0.3, 0.1, 0.1, 0.1])\n",
    "# Générations des *blobs*\n",
    "X, y = make_blobs(n_samples=2000, centers=blob_centers, cluster_std=blob_std, random_state=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Représentation graphique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_clusters(X, y=None):\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, s=1)\n",
    "    plt.xlabel(\"$x_1$\", fontsize=14)\n",
    "    plt.ylabel(\"$x_2$\", fontsize=14, rotation=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "plot_clusters(X)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Exécution de *K-means*\n",
    "Le nombre de classes est supposé *a priori* connu. Ce choix est discuté plus loin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n",
    "# Choix des options\n",
    "kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "# Exécution de l'algorithme\n",
    "y_pred = kmeans.fit_predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# affichage des classes\n",
    "kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# qui sont aussi dans la variables `y_pred`\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction `fit_predict` calcule aussi l'affectation à la classe dont le barycentre est le plus proche pour de nouvelles observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = np.array([[0, 2], [3, 2], [-3, 3], [-3, 2.5]])\n",
    "kmeans.predict(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction `transform` calcule elle les distances aux barycentres plutôt que l'affectation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.transform(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Affichage des barycentres des classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Frontières des classes\n",
    "Le tracé des frontières des classes profiut un diagramme de Voronoi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(X):\n",
    "    plt.plot(X[:, 0], X[:, 1], 'k.', markersize=2)\n",
    "\n",
    "def plot_centroids(centroids, weights=None, circle_color='w', cross_color='k'):\n",
    "    if weights is not None:\n",
    "        centroids = centroids[weights > weights.max() / 10]\n",
    "    plt.scatter(centroids[:, 0], centroids[:, 1],\n",
    "                marker='o', s=30, linewidths=8,\n",
    "                color=circle_color, zorder=10, alpha=0.9)\n",
    "    plt.scatter(centroids[:, 0], centroids[:, 1],\n",
    "                marker='x', s=50, linewidths=50,\n",
    "                color=cross_color, zorder=11, alpha=1)\n",
    "\n",
    "def plot_decision_boundaries(clusterer, X, resolution=1000, show_centroids=True,\n",
    "                             show_xlabels=True, show_ylabels=True):\n",
    "    mins = X.min(axis=0) - 0.1\n",
    "    maxs = X.max(axis=0) + 0.1\n",
    "    xx, yy = np.meshgrid(np.linspace(mins[0], maxs[0], resolution),\n",
    "                         np.linspace(mins[1], maxs[1], resolution))\n",
    "    Z = clusterer.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    plt.contourf(Z, extent=(mins[0], maxs[0], mins[1], maxs[1]),\n",
    "                cmap=\"Pastel2\")\n",
    "    plt.contour(Z, extent=(mins[0], maxs[0], mins[1], maxs[1]),\n",
    "                linewidths=1, colors='k')\n",
    "    plot_data(X)\n",
    "    if show_centroids:\n",
    "        plot_centroids(clusterer.cluster_centers_)\n",
    "\n",
    "    if show_xlabels:\n",
    "        plt.xlabel(\"$x_1$\", fontsize=14)\n",
    "    else:\n",
    "        plt.tick_params(labelbottom=False)\n",
    "    if show_ylabels:\n",
    "        plt.ylabel(\"$x_2$\", fontsize=14, rotation=0)\n",
    "    else:\n",
    "        plt.tick_params(labelleft=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "plot_decision_boundaries(kmeans, X)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce graphique montre bien que l'algorithme est typiquement basé sur des distances euclidiennes et ne prend pas en compte la dispersion plus ou moins grande des classes. Certains points à la frontière ne semblent pas affectés à la bonne classe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Exécution multiple\n",
    "Comme la convergence de *k-means* est locale, la variabilité des classes obtenues peut être évaluée en considérant plusieurs exécutions de l'algorithme à partir d'initialisation aléatoires différentes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_clusterer_comparison(clusterer1, clusterer2, X, title1=None, title2=None):\n",
    "    clusterer1.fit(X)\n",
    "    clusterer2.fit(X)\n",
    "\n",
    "    plt.figure(figsize=(10, 3.2))\n",
    "\n",
    "    plt.subplot(121)\n",
    "    plot_decision_boundaries(clusterer1, X)\n",
    "    if title1:\n",
    "        plt.title(title1, fontsize=14)\n",
    "\n",
    "    plt.subplot(122)\n",
    "    plot_decision_boundaries(clusterer2, X, show_ylabels=False)\n",
    "    if title2:\n",
    "        plt.title(title2, fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chaque exécution est limitée à une seule initilisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_rnd_init1 = KMeans(n_clusters=5, init=\"random\", n_init=1, algorithm=\"full\", random_state=11)\n",
    "kmeans_rnd_init2 = KMeans(n_clusters=5, init=\"random\", n_init=1, algorithm=\"full\", random_state=19)\n",
    "\n",
    "plot_clusterer_comparison(kmeans_rnd_init1, kmeans_rnd_init2, X,\n",
    "                          \"Solution 1\", \"Solution 2 avec différentes initialisations\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'évaluation de l'inertie du nuage permet de comparer les qualités de ces classifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.inertia_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inertie qui est la somme des carrés des distances entre chaque observation et le barycentre de sa classe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dist = kmeans.transform(X)\n",
    "np.sum(X_dist[np.arange(len(X_dist)), kmeans.labels_]**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.score(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alors que le `score()` est moins l'inertie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une meilleure solution serait celle qui minimise l'inertie. La première exécution est donc meilleure car par défaut l'algorithme lance 10 exécutions  et conserve celle qui minimise l'inertie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_rnd_init1.inertia_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_rnd_init2.inertia_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exécution avec 10 initialisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_rnd_10_inits = KMeans(n_clusters=5, init=\"random\", n_init=10, algorithm=\"full\", random_state=11)\n",
    "kmeans_rnd_10_inits.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "qui fournit sans doute la meilleure classification connaissant le bon nombre de classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "plot_decision_boundaries(kmeans_rnd_10_inits, X)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 *K-means++*\n",
    "[Arthur and Vassilvitskii (2006)](https://goo.gl/eNUPw6) ont proposé une autre initialisation des barycentres de l'algorithme:\n",
    "* Choisir un centre $c_1$ aléatoirement, univormément dans les données, \n",
    "* Répéter $K-1$ fois: choisir un autre centre $c_k$, parmi des les données: $\\mathbf{x}_i$ avec la probabilité $D(\\mathbf{x}_i)^2$ / $\\sum\\limits_{j=1}^{m}{D(\\mathbf{x}_j)}^2$ où $D(\\mathbf{x}_i)$ est la distance entre $\\mathbf{x}_i$ et le centre le plus proche déjà choisi. Cette probabilité assure que le nouveau centre est éloigné des autres centres précédemment choisis.\n",
    "\n",
    "En principe ce procédé permet de réduire le nombre `n_init` d'initialisations et de donc de compenser le temps pris par cette procédure d'initialisation. \n",
    "\n",
    "Remarque: cette initialisation est obtenue par `init=\"k-means++\"` qui est prise par défaut. Une \"initialisation optimale\"conduit à des résultats identiques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KMeans()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_init = np.array([[-3, 3], [-3, 2], [-3, 1], [-1, 2], [0, 2]])\n",
    "kmeans = KMeans(n_clusters=5, init=good_init, n_init=1, random_state=42)\n",
    "kmeans.fit(X)\n",
    "kmeans.inertia_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Accélération de *K-means*\n",
    "[Elkan (2003)](https://www.aaai.org/Papers/ICML/2003/ICML03-022.pdf) a proposé une accélération de l'algorithme en exploitant l'inégalité triangulaire pour économiser des calculs de distance: pour trois points A, B et C, la distance AC ≤ AB + BC. Cela permet de garder en mémoire des bornes inféreures et supérieures des distances entre observations et barycentres. L'option `algorithm=\"elkan\"` exécute ce procédé par défaut mais seulement pour des données denses. Pour des données creuses (*sparse*) l'option `full` est utilisée par défaut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit -n 50 KMeans(algorithm=\"elkan\").fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%timeit -n 50 KMeans(algorithm=\"full\").fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Optimiser le nombre de classes\n",
    "Un aute nombre de classes que celui initial par construction perturbe l'agorithme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_k3 = KMeans(n_clusters=3, random_state=42)\n",
    "kmeans_k8 = KMeans(n_clusters=8, random_state=42)\n",
    "\n",
    "plot_clusterer_comparison(kmeans_k3, kmeans_k8, X, \"$K=3$\", \"$K=8$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_k3.inertia_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_k8.inertia_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mais le critère d'inertie ne permet pas de choisir car celle-ci décroît mécaniquement avec le nombre de classes. Nénamoins la recherche d'un coude dans cette décroissance peut être une indication d'un nombre minimal de classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_per_k = [KMeans(n_clusters=k, random_state=42).fit(X)\n",
    "                for k in range(1, 10)]\n",
    "inertias = [model.inertia_ for model in kmeans_per_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 3.5))\n",
    "plt.plot(range(1, 10), inertias, \"bo-\")\n",
    "plt.xlabel(\"$k$\", fontsize=14)\n",
    "plt.ylabel(\"Inertia\", fontsize=14)\n",
    "plt.axis([1, 8.5, 0, 1300])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention, ces données \"jouets\" sont relativement \"faciles\" à segmenter en classes. des données réelles ne conduisent pas à des graphes aussi limpides. Le choix de 4 classes n'est pas mauvais en soit; la principale difficulté pour segmenter ces données réside dans les dispersions différentes des groupes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tracé des silouhettes des classes en fonction de $K$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouette_score(X, kmeans.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouette_scores = [silhouette_score(X, model.labels_)\n",
    "                     for model in kmeans_per_k[1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_boundaries(kmeans_per_k[4-1], X)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 3))\n",
    "plt.plot(range(2, 10), silhouette_scores, \"bo-\")\n",
    "plt.xlabel(\"$k$\", fontsize=14)\n",
    "plt.ylabel(\"Silhouette score\", fontsize=14)\n",
    "plt.axis([1.8, 8.5, 0.55, 0.7])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_samples\n",
    "from matplotlib.ticker import FixedLocator, FixedFormatter\n",
    "\n",
    "plt.figure(figsize=(11, 9))\n",
    "\n",
    "for k in (3, 4, 5, 6):\n",
    "    plt.subplot(2, 2, k - 2)\n",
    "    \n",
    "    y_pred = kmeans_per_k[k - 1].labels_\n",
    "    silhouette_coefficients = silhouette_samples(X, y_pred)\n",
    "\n",
    "    padding = len(X) // 30\n",
    "    pos = padding\n",
    "    ticks = []\n",
    "    for i in range(k):\n",
    "        coeffs = silhouette_coefficients[y_pred == i]\n",
    "        coeffs.sort()\n",
    "        cmap = matplotlib.cm.get_cmap(\"Spectral\")\n",
    "        color = cmap(i / k)\n",
    "        plt.fill_betweenx(np.arange(pos, pos + len(coeffs)), 0, coeffs,\n",
    "                          facecolor=color, edgecolor=color, alpha=0.7)\n",
    "        ticks.append(pos + len(coeffs) // 2)\n",
    "        pos += len(coeffs) + padding\n",
    "\n",
    "    plt.gca().yaxis.set_major_locator(FixedLocator(ticks))\n",
    "    plt.gca().yaxis.set_major_formatter(FixedFormatter(range(k)))\n",
    "    if k in (3, 5):\n",
    "        plt.ylabel(\"Cluster\")\n",
    "    \n",
    "    if k in (5, 6):\n",
    "        plt.gca().set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "        plt.xlabel(\"Silhouette Coefficient\")\n",
    "    else:\n",
    "        plt.tick_params(labelbottom=False)\n",
    "\n",
    "    plt.axvline(x=silhouette_scores[k - 2], color=\"red\", linestyle=\"--\")\n",
    "    plt.title(\"$k={}$\".format(k), fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7 Limites de *K-means*\n",
    "Cet algorithe favorise des classes sphériques. Il n'est donc pas adapté à des classes aux formes exotiques ni même elliptiques. Deux classifications sont comparées dont une, bonne, initialisée avec les centres connus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1, y1 = make_blobs(n_samples=1000, centers=((4, -4), (0, 0)), random_state=42)\n",
    "X1 = X1.dot(np.array([[0.374, 0.95], [0.732, 0.598]]))\n",
    "X2, y2 = make_blobs(n_samples=250, centers=1, random_state=42)\n",
    "X2 = X2 + [6, -8]\n",
    "X = np.r_[X1, X2]\n",
    "y = np.r_[y1, y2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_clusters(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_good = KMeans(n_clusters=3, init=np.array([[-1.5, 2.5], [0.5, 0], [4, 0]]), n_init=1, random_state=42)\n",
    "kmeans_bad = KMeans(n_clusters=3, random_state=42)\n",
    "kmeans_good.fit(X)\n",
    "kmeans_bad.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 3.2))\n",
    "\n",
    "plt.subplot(121)\n",
    "plot_decision_boundaries(kmeans_good, X)\n",
    "plt.title(\"Inertia = {:.1f}\".format(kmeans_good.inertia_), fontsize=14)\n",
    "\n",
    "plt.subplot(122)\n",
    "plot_decision_boundaries(kmeans_bad, X, show_ylabels=False)\n",
    "plt.title(\"Inertia = {:.1f}\".format(kmeans_bad.inertia_), fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 DBSCAN\n",
    "ALgorithmes plus particulièrement adapté à des classes bien distinctes même si de formes très exotiques. Il marche donc particulièrement bien sur des données synthétiques. Le choix des valeurs des paramètres: `eps, min_samples` de cet algorithme est délicat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_moons(n_samples=1000, noise=0.05, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan = DBSCAN(eps=0.05, min_samples=5)\n",
    "dbscan.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan.labels_[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dbscan.core_sample_indices_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan.core_sample_indices_[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan.components_[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(dbscan.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan2 = DBSCAN(eps=0.2)\n",
    "dbscan2.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dbscan(dbscan, X, size, show_xlabels=True, show_ylabels=True):\n",
    "    core_mask = np.zeros_like(dbscan.labels_, dtype=bool)\n",
    "    core_mask[dbscan.core_sample_indices_] = True\n",
    "    anomalies_mask = dbscan.labels_ == -1\n",
    "    non_core_mask = ~(core_mask | anomalies_mask)\n",
    "\n",
    "    cores = dbscan.components_\n",
    "    anomalies = X[anomalies_mask]\n",
    "    non_cores = X[non_core_mask]\n",
    "    \n",
    "    plt.scatter(cores[:, 0], cores[:, 1],\n",
    "                c=dbscan.labels_[core_mask], marker='o', s=size, cmap=\"Paired\")\n",
    "    plt.scatter(cores[:, 0], cores[:, 1], marker='*', s=20, c=dbscan.labels_[core_mask])\n",
    "    plt.scatter(anomalies[:, 0], anomalies[:, 1],\n",
    "                c=\"r\", marker=\"x\", s=100)\n",
    "    plt.scatter(non_cores[:, 0], non_cores[:, 1], c=dbscan.labels_[non_core_mask], marker=\".\")\n",
    "    if show_xlabels:\n",
    "        plt.xlabel(\"$x_1$\", fontsize=14)\n",
    "    else:\n",
    "        plt.tick_params(labelbottom=False)\n",
    "    if show_ylabels:\n",
    "        plt.ylabel(\"$x_2$\", fontsize=14, rotation=0)\n",
    "    else:\n",
    "        plt.tick_params(labelleft=False)\n",
    "    plt.title(\"eps={:.2f}, min_samples={}\".format(dbscan.eps, dbscan.min_samples), fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dbscan(dbscan, X, size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9, 3.2))\n",
    "\n",
    "plt.subplot(121)\n",
    "plot_dbscan(dbscan, X, size=100)\n",
    "\n",
    "plt.subplot(122)\n",
    "plot_dbscan(dbscan2, X, size=600, show_ylabels=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 Mélange gaussien\n",
    "\n",
    "### 6.1 Iris de Fisher\n",
    "Exemple typique sur des données classiques.\n",
    "![Iris](03_iris.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "# from IPython.display import IFrame\n",
    "# IFrame('http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data', width=300, height=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "data.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9, 3.5))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.plot(X[y==0, 0], X[y==0, 1], \"yo\", label=\"Iris-Setosa\")\n",
    "plt.plot(X[y==1, 0], X[y==1, 1], \"bs\", label=\"Iris-Versicolor\")\n",
    "plt.plot(X[y==2, 0], X[y==2, 1], \"g^\", label=\"Iris-Virginica\")\n",
    "plt.xlabel(\"Petal length\", fontsize=14)\n",
    "plt.ylabel(\"Petal width\", fontsize=14)\n",
    "plt.legend(fontsize=12)\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=\"k\", marker=\".\")\n",
    "plt.xlabel(\"Petal length\", fontsize=14)\n",
    "plt.tick_params(labelleft=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un mélange gaussien est bien adapté à ces données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = GaussianMixture(n_components=3, random_state=42).fit(X).predict(X)\n",
    "mapping = np.array([2, 0, 1])\n",
    "y_pred = np.array([mapping[cluster_id] for cluster_id in y_pred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X[y_pred==0, 0], X[y_pred==0, 1], \"yo\", label=\"Cluster 1\")\n",
    "plt.plot(X[y_pred==1, 0], X[y_pred==1, 1], \"bs\", label=\"Cluster 2\")\n",
    "plt.plot(X[y_pred==2, 0], X[y_pred==2, 1], \"g^\", label=\"Cluster 3\")\n",
    "plt.xlabel(\"Petal length\", fontsize=14)\n",
    "plt.ylabel(\"Petal width\", fontsize=14)\n",
    "plt.legend(loc=\"upper right\", fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Mélange gaussien des blobs\n",
    "Cet algorithme est particulièrement adapté aux données précédemment générées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(y_pred==y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(y_pred==y) / len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1, y1 = make_blobs(n_samples=1000, centers=((4, -4), (0, 0)), random_state=42)\n",
    "X1 = X1.dot(np.array([[0.374, 0.95], [0.732, 0.598]]))\n",
    "X2, y2 = make_blobs(n_samples=250, centers=1, random_state=42)\n",
    "X2 = X2 + [6, -8]\n",
    "X = np.r_[X1, X2]\n",
    "y = np.r_[y1, y2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gm = GaussianMixture(n_components=3, n_init=10, random_state=42)\n",
    "gm.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Valeurs des paramètres estimés par l'algorithme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gm.weights_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gm.means_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gm.covariances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La convergence est elle atteinte?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gm.converged_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En combien d'itérations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gm.n_iter_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction `predict()` permet de prédire l'appartenance d'une observation à une classe ou encore avec  `predict_proba()`, la probabilité d'appartenance à chacune des classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gm.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gm.predict_proba(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De même que pour de nouvelles observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new, y_new = gm.sample(6)\n",
    "X_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimation du logarithme de la fonction de densité entout point par `score_samples()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gm.score_samples(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graphique des frontières des classes et des contours des densités."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import LogNorm\n",
    "\n",
    "def plot_gaussian_mixture(clusterer, X, resolution=1000, show_ylabels=True):\n",
    "    mins = X.min(axis=0) - 0.1\n",
    "    maxs = X.max(axis=0) + 0.1\n",
    "    xx, yy = np.meshgrid(np.linspace(mins[0], maxs[0], resolution),\n",
    "                         np.linspace(mins[1], maxs[1], resolution))\n",
    "    Z = -clusterer.score_samples(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    plt.contourf(xx, yy, Z,\n",
    "                 norm=LogNorm(vmin=1.0, vmax=30.0),\n",
    "                 levels=np.logspace(0, 2, 12))\n",
    "    plt.contour(xx, yy, Z,\n",
    "                norm=LogNorm(vmin=1.0, vmax=30.0),\n",
    "                levels=np.logspace(0, 2, 12),\n",
    "                linewidths=1, colors='k')\n",
    "\n",
    "    Z = clusterer.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.contour(xx, yy, Z,\n",
    "                linewidths=2, colors='r', linestyles='dashed')\n",
    "    \n",
    "    plt.plot(X[:, 0], X[:, 1], 'k.', markersize=2)\n",
    "    plot_centroids(clusterer.means_, clusterer.weights_)\n",
    "\n",
    "    plt.xlabel(\"$x_1$\", fontsize=14)\n",
    "    if show_ylabels:\n",
    "        plt.ylabel(\"$x_2$\", fontsize=14, rotation=0)\n",
    "    else:\n",
    "        plt.tick_params(labelleft=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "plot_gaussian_mixture(gm, X)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En vue d'estimer des modèles plus parcimonieux, c'est-à-dire avec moins de paramètres, des contraintes peuvent être imposées aux matrices de covariance en spécifiant le paramètre `covariance_type`:\n",
    "\n",
    "* `\"full\"` (defaut): pas de contrainte sur les matrices de covariance, chaque ellipsoïde peut avoir des formes spécifiques, \n",
    "* `\"tied\"`: même matrices de covariance ou même forme des nauges,\n",
    "* `\"spherical\"`: nuages sphériques mais de diamètres différents,\n",
    "* `\"diag\"`: nuages ellipsoïdaux parallèles mais de dimensions différentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gm_full = GaussianMixture(n_components=3, n_init=10, covariance_type=\"full\", random_state=42)\n",
    "gm_tied = GaussianMixture(n_components=3, n_init=10, covariance_type=\"tied\", random_state=42)\n",
    "gm_spherical = GaussianMixture(n_components=3, n_init=10, covariance_type=\"spherical\", random_state=42)\n",
    "gm_diag = GaussianMixture(n_components=3, n_init=10, covariance_type=\"diag\", random_state=42)\n",
    "gm_full.fit(X)\n",
    "gm_tied.fit(X)\n",
    "gm_spherical.fit(X)\n",
    "gm_diag.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_gaussian_mixtures(gm1, gm2, X):\n",
    "    plt.figure(figsize=(9, 4))\n",
    "\n",
    "    plt.subplot(121)\n",
    "    plot_gaussian_mixture(gm1, X)\n",
    "    plt.title('covariance_type=\"{}\"'.format(gm1.covariance_type), fontsize=14)\n",
    "\n",
    "    plt.subplot(122)\n",
    "    plot_gaussian_mixture(gm2, X, show_ylabels=False)\n",
    "    plt.title('covariance_type=\"{}\"'.format(gm2.covariance_type), fontsize=14)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_gaussian_mixtures(gm_tied, gm_spherical, X)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_gaussian_mixtures(gm_full, gm_diag, X)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparer ces graphiques, lequel semble le plus adapté à la forme des nuages?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Détection d'anomalies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme la clasisficaiton ascendante hiérarchique, les mélanges gaussiens peuvent être utilisés pour [détecter des anomalies]() ou observations atypiques c'est-à-dire présentes dans des zones de faible densité. Un paramètre doit être réglé, il définit par exemple le taux d'anomalies à détecter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "densities = gm.score_samples(X)\n",
    "density_threshold = np.percentile(densities, 4)\n",
    "anomalies = X[densities < density_threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "plot_gaussian_mixture(gm, X)\n",
    "plt.scatter(anomalies[:, 0], anomalies[:, 1], color='r', marker='*')\n",
    "plt.ylim(ymax=5.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Sélection de modèle\n",
    "L'inertie, qui décroît avec le nombre de paramètres du modèles donc de classes, de même que le score de silouhette adapté à des nuages sphériques ne sont pas adaptés à l'optimisation dex choix de modèles: nombre de classes et contraintes sur les matrices de covariance. Le modèle de mélange est un modèle statistique usuel estimé par maximum de la log vraisemblance. La stratégie de sélection de modèle par pénalisation de la vraisemblance du modèle linéaire général peut être utilisée pour le modèle de mélange gaussien. Cette sytratégie conduit à la minimisation des critères BIC *(Bayesian Information Criterion)* et AIC *(Akaike Information Criterion)* qui introduisent une pénalisation fonction du nombre de paramètres.\n",
    "\n",
    "${BIC} = {\\log(m)p - 2\\log({\\hat L})}$\n",
    "\n",
    "${AIC} = 2p - 2\\log(\\hat L)$\n",
    "\n",
    "* $m$ est la taille de l'échantillon,\n",
    "* $p$ est le nombre de paramètres estimés dans le modèle: moyennes, éventuellement les matrices de covariance,\n",
    "* $\\hat L$ la log vraisemblance du modèle minimisée.\n",
    "\n",
    "Minimiser le BIC ou l'AIC conduit trouver un \"meilleur\" équilibre entre qualité d'ajustement du modèle (Biais) et variance des paramètres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gm.bic(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gm.aic(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train Gaussian Mixture models with various values of $k$ and measure their BIC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gms_per_k = [GaussianMixture(n_components=k, n_init=10, random_state=42).fit(X)\n",
    "             for k in range(1, 11)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bics = [model.bic(X) for model in gms_per_k]\n",
    "aics = [model.aic(X) for model in gms_per_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 3))\n",
    "plt.plot(range(1, 11), bics, \"bo-\", label=\"BIC\")\n",
    "plt.plot(range(1, 11), aics, \"go--\", label=\"AIC\")\n",
    "plt.xlabel(\"$k$\", fontsize=14)\n",
    "plt.ylabel(\"Information Criterion\", fontsize=14)\n",
    "plt.axis([1, 9.5, np.min(aics) - 50, np.max(aics) + 50])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chercher le meilleur choix pour le nombre de classes et le `covariance_type`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_bic = np.infty\n",
    "\n",
    "for k in range(1, 11):\n",
    "    for covariance_type in (\"full\", \"tied\", \"spherical\", \"diag\"):\n",
    "        bic = GaussianMixture(n_components=k, n_init=10,\n",
    "                              covariance_type=covariance_type,\n",
    "                              random_state=42).fit(X).bic(X)\n",
    "        if bic < min_bic:\n",
    "            min_bic = bic\n",
    "            best_k = k\n",
    "            best_covariance_type = covariance_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_covariance_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
